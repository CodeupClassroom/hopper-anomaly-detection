{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Anomalies Using Density Based Clustering\n",
    "\n",
    "\n",
    "## Clustering-Based Anomaly Detection\n",
    "\n",
    "- Assumption: Data points that are similar tend to belong to similar groups or clusters, as determined by their distance from local centroids. Normal data points occur around a dense neighborhood and abnormalities are far away.\n",
    "\n",
    "- Using density based clustering, like DBSCAN, we can design the model such that the data points that do not fall into a cluster are the anomalies.\n",
    "\n",
    "https://docs.google.com/presentation/d/1frjnm1ii63gcFSyhQxxzvekrMAAO0xnw-aitdTPIbc0/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# DBSCAN import\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Scaler import\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curriculum_logs():\n",
    "    filename = \"curriculum-access.csv\"\n",
    "\n",
    "    if os.path.isfile(filename):\n",
    "        return pd.read_csv(filename, index_col=False)\n",
    "    else:\n",
    "        # read the SQL query into a dataframe\n",
    "        url = f'mysql+pymysql://{env.user}:{env.password}@{env.host}/curriculum_logs'\n",
    "        query = '''\n",
    "        SELECT date,\n",
    "               path as endpoint,\n",
    "               user_id,\n",
    "               cohort_id,\n",
    "               ip as source_ip\n",
    "        FROM logs;\n",
    "        '''\n",
    "        df = pd.read_sql(query, url)\n",
    "\n",
    "        # Write that dataframe to disk for later.\n",
    "        df.to_csv(filename, index = False)\n",
    "\n",
    "        return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire data using the above function\n",
    "df = get_curriculum_logs()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to a pandas datetime format and set as index\n",
    "df.date = pd.to_datetime(df.date)\n",
    "df = df.set_index(df.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who is accessing the curriculum a lot (total views) and possibiliy looking at lot of unique pages (scraping?) \n",
    "\n",
    "Aggregate and compute 2 features...number of unique pages and total page views. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views = df.groupby(['user_id'])['endpoint'].agg(['count', 'nunique'])\n",
    "page_views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale each attribute linearly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the scaler\n",
    "scaler = MinMaxScaler().fit(page_views)\n",
    "# use the scaler\n",
    "page_views_scaled_array = scaler.transform(page_views)\n",
    "page_views_scaled_array[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a DBSCAN object that requires a minimum of 4 data points in a neighborhood of radius 0.1 to be considered a core point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbsc = DBSCAN(eps = 0.1, min_samples=4).fit(page_views_scaled_array)\n",
    "print(dbsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's add the scaled value columns back onto the dataframe\n",
    "\n",
    "columns = list(page_views.columns)\n",
    "scaled_columns = [\"scaled_\" + column for column in columns]\n",
    "scaled_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe containing the scaled values\n",
    "scaled_df = pd.DataFrame(page_views_scaled_array, columns=scaled_columns, index=page_views.index)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the scaled and non-scaled values into one dataframe\n",
    "page_views = page_views.merge(scaled_df, left_index=True, right_index=True)\n",
    "page_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dbsc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label '-1' represents observations which are not part of any cluster and are potential anomalies warranting further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add labels back to the dataframe\n",
    "page_views['labels'] = labels\n",
    "\n",
    "# how many unique labels (clusters) are created by DBSCAN?\n",
    "page_views.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views[page_views.labels==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(page_views['scaled_count'], page_views['scaled_nunique'], c=page_views.labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with the DBSCAN properties\n",
    "- Read up on the epsilon and min_samples arguments into DBSCAN at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "- Experiment with altering the epsilon values (the `eps` argument holding the threshhold parameter). Run the models and visualize the results. What has changed? Why do you think that is?\n",
    "- Double the `min_samples` parameter. Run your model and visualize the results. Consider what changed and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "**file name:** clustering_anomaly_detection.py or clustering_anomaly_detection.ipynb\n",
    "\n",
    "\n",
    "### Clustering - DBSCAN\n",
    "\n",
    "Ideas: \n",
    "\n",
    "Use DBSCAN to detect anomalies in curriculumn access. \n",
    "\n",
    "Use DBSCAN to detect anomalies in other products from the customers dataset. \n",
    "\n",
    "Use DBSCAN to detect anomalies in number of bedrooms and finished square feet of property for the filtered dataset you used in the clustering project (single unit properties with a logerror).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
